---
title: "PaperReading: MLLM_5 - Multimodal Chain-of-Thought"
timestamp: 2026-01-31 00:00:00+08:00
series: PaperReading
tags: [MLLM, Paper, CoT]
description: 多模态思维链相关论文阅读。
---

### Description

> [!NOTE] 普通论文
> 标准论文记录,蓝色

> [!TIP] 推荐阅读
> 特别推荐的经典论文,绿色

> [!IMPORTANT] 重要突破
> 里程碑式的重要论文,紫色

> [!WARNING] 需注意
> 有争议或需要特别注意的论文,橙色

<br>

## Insight-V

> [!NOTE] Insight-V
> **Arxiv** [2411.14432](https://arxiv.org/abs/2411.14432)
>
> **翻译** [2411.14432](https://hjfy.top/arxiv/2411.14432)
>
> **代码** [Github](https://github.com/dongyh20/Insight-V)

## Cantor

> [!NOTE] Cantor
> **Arxiv** [2404.16033](https://arxiv.org/abs/2404.16033)
>
> **翻译** [2404.16033](https://hjfy.top/arxiv/2404.16033)
>
> **代码** [Github](https://github.com/ggg0919/cantor)

## Visual CoT

> [!NOTE] Visual CoT
> **Arxiv** [2403.16999](https://arxiv.org/abs/2403.16999)
>
> **翻译** [2403.16999](https://hjfy.top/arxiv/2403.16999)
>
> **代码** [Github](https://github.com/deepcs233/Visual-CoT)

## CCoT

> [!NOTE] CCoT
> **Arxiv** [2311.17076](https://arxiv.org/abs/2311.17076)
>
> **翻译** [2311.17076](https://hjfy.top/arxiv/2311.17076)
>
> **代码** [Github](https://github.com/chancharikmitra/CCoT)

## DDCoT

> [!NOTE] DDCoT
> **Arxiv** [2310.16436](https://arxiv.org/abs/2310.16436)
>
> **翻译** [2310.16436](https://hjfy.top/arxiv/2310.16436)
>
> **代码** [Github](https://github.com/SooLab/DDCOT)

## Shikra

> [!NOTE] Shikra
> **Arxiv** [2306.15195](https://arxiv.org/abs/2306.15195)
>
> **翻译** [2306.15195](https://hjfy.top/arxiv/2306.15195)
>
> **代码** [Github](https://github.com/shikras/shikra)
>
> **Demo** [Demo](http://demo.zhaozhang.net:7860/)

## Explainable Multimodal Emotion Reasoning

> [!NOTE] EMER
> **Arxiv** [2306.15401](https://arxiv.org/abs/2306.15401)
>
> **翻译** [2306.15401](https://hjfy.top/arxiv/2306.15401)
>
> **代码** [Github](https://github.com/zeroQiaoba/Explainable-Multimodal-Emotion-Reasoning)

## EmbodiedGPT

> [!NOTE] EmbodiedGPT
> **Arxiv** [2305.15021](https://arxiv.org/abs/2305.15021)
>
> **翻译** [2305.15021](https://hjfy.top/arxiv/2305.15021)
>
> **代码** [Github](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)

## Video Chain of Thought

> [!NOTE] Video CoT
> **Arxiv** [2305.13903](https://arxiv.org/abs/2305.13903)
>
> **翻译** [2305.13903](https://hjfy.top/arxiv/2305.13903)

## T-SciQ

> [!NOTE] T-SciQ
> **Arxiv** [2305.03453](https://arxiv.org/abs/2305.03453)
>
> **翻译** [2305.03453](https://hjfy.top/arxiv/2305.03453)

## Caption Anything

> [!NOTE] Caption Anything
> **Arxiv** [2305.02677](https://arxiv.org/abs/2305.02677)
>
> **翻译** [2305.02677](https://hjfy.top/arxiv/2305.02677)
>
> **代码** [Github](https://github.com/ttengwang/Caption-Anything)
>
> **Demo** [Demo](https://huggingface.co/spaces/TencentARC/Caption-Anything)

## Visual Chain of Thought

> [!NOTE] VCoT
> **Arxiv** [2305.02317](https://arxiv.org/abs/2305.02317)
>
> **翻译** [2305.02317](https://hjfy.top/arxiv/2305.02317)

## Chain of Thought Prompt Tuning

> [!NOTE] CoT Prompt Tuning
> **Arxiv** [2304.07919](https://arxiv.org/abs/2304.07919)
>
> **翻译** [2304.07919](https://hjfy.top/arxiv/2304.07919)

## Visual ChatGPT

> [!NOTE] Visual ChatGPT
> **Arxiv** [2303.04671](https://arxiv.org/abs/2303.04671)
>
> **翻译** [2303.04671](https://hjfy.top/arxiv/2303.04671)
>
> **代码** [Github](https://github.com/microsoft/TaskMatrix)
>
> **Demo** [Demo](https://huggingface.co/spaces/microsoft/visual_chatgpt)

## Multimodal-CoT

> [!IMPORTANT] MM-CoT
> **Arxiv** [2302.00923](https://arxiv.org/abs/2302.00923)
>
> **翻译** [2302.00923](https://hjfy.top/arxiv/2302.00923)
>
> **代码** [Github](https://github.com/amazon-science/mm-cot)

## ScienceQA

> [!IMPORTANT] ScienceQA
> **Arxiv** [2209.09513](https://arxiv.org/abs/2209.09513)
>
> **翻译** [2209.09513](https://hjfy.top/arxiv/2209.09513)
>
> **代码** [Github](https://github.com/lupantech/ScienceQA)

> [!IMPORTANT] 待读论文
> 本文档包含多模态思维链相关的待读论文列表,持续更新中...
