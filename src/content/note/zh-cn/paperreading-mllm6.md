---
title: "PaperReading: MLLM_6 - LLM-Aided Visual Reasoning"
timestamp: 2026-01-31 00:00:00+08:00
series: PaperReading
tags: [MLLM, Paper, Visual Reasoning]
description: LLM辅助视觉推理相关论文阅读。
---

### Description

> [!NOTE] 普通论文
> 标准论文记录,蓝色

> [!TIP] 推荐阅读
> 特别推荐的经典论文,绿色

> [!IMPORTANT] 重要突破
> 里程碑式的重要论文,紫色

> [!WARNING] 需注意
> 有争议或需要特别注意的论文,橙色

<br>

## VideoDeepResearch

> [!NOTE] VideoDeepResearch
> **Arxiv** [2506.10821](https://arxiv.org/abs/2506.10821)
>
> **翻译** [2506.10821](https://hjfy.top/arxiv/2506.10821)
>
> **代码** [Github](https://github.com/yhy-2000/VideoDeepResearch)

## Visual Table

> [!NOTE] Visual Table
> **Arxiv** [2403.18252](https://arxiv.org/abs/2403.18252)
>
> **翻译** [2403.18252](https://hjfy.top/arxiv/2403.18252)
>
> **代码** [Github](https://github.com/LaVi-Lab/Visual-Table)

## V\*

> [!NOTE] V-Star
> **Arxiv** [2312.14135](https://arxiv.org/abs/2312.14135)
>
> **翻译** [2312.14135](https://hjfy.top/arxiv/2312.14135)
>
> **代码** [Github](https://github.com/penghao-wu/vstar)

## LLaVA-Interactive

> [!NOTE] LLaVA-Interactive
> **Arxiv** [2311.00571](https://arxiv.org/abs/2311.00571)
>
> **翻译** [2311.00571](https://hjfy.top/arxiv/2311.00571)
>
> **代码** [Github](https://github.com/LLaVA-VL/LLaVA-Interactive-Demo)
>
> **Demo** [Demo](https://6dd3-20-163-117-69.ngrok-free.app/)

## MM-VID

> [!NOTE] MM-VID
> **Arxiv** [2310.19773](https://arxiv.org/abs/2310.19773)
>
> **翻译** [2310.19773](https://hjfy.top/arxiv/2310.19773)

## ControlLLM

> [!NOTE] ControlLLM
> **Arxiv** [2310.17796](https://arxiv.org/abs/2310.17796)
>
> **翻译** [2310.17796](https://hjfy.top/arxiv/2310.17796)
>
> **代码** [Github](https://github.com/OpenGVLab/ControlLLM)

## MindAgent

> [!NOTE] MindAgent
> **Arxiv** [2309.09971](https://arxiv.org/abs/2309.09971)
>
> **翻译** [2309.09971](https://hjfy.top/arxiv/2309.09971)
>
> **代码** [Github](https://github.com/mindagent/mindagent)

## LENS

> [!NOTE] LENS
> **Arxiv** [2306.16410](https://arxiv.org/abs/2306.16410)
>
> **翻译** [2306.16410](https://hjfy.top/arxiv/2306.16410)
>
> **代码** [Github](https://github.com/ContextualAI/lens)
>
> **Demo** [Demo](https://lens.contextual.ai/)

## Retrieving-to-Answer

> [!NOTE] RTA
> **Arxiv** [2306.11732](https://arxiv.org/abs/2306.11732)
>
> **翻译** [2306.11732](https://hjfy.top/arxiv/2306.11732)

## AssistGPT

> [!NOTE] AssistGPT
> **Arxiv** [2306.08640](https://arxiv.org/abs/2306.08640)
>
> **翻译** [2306.08640](https://hjfy.top/arxiv/2306.08640)
>
> **代码** [Github](https://github.com/showlab/assistgpt)

## GPT4Tools

> [!NOTE] GPT4Tools
> **Arxiv** [2305.18752](https://arxiv.org/abs/2305.18752)
>
> **翻译** [2305.18752](https://hjfy.top/arxiv/2305.18752)
>
> **代码** [Github](https://github.com/StevenGrove/GPT4Tools)
>
> **Demo** [Demo](https://c60eb7e9400930f31b.gradio.live/)

## Mindstorms

> [!NOTE] Mindstorms
> **Arxiv** [2305.17066](https://arxiv.org/abs/2305.17066)
>
> **翻译** [2305.17066](https://hjfy.top/arxiv/2305.17066)

## LayoutGPT

> [!NOTE] LayoutGPT
> **Arxiv** [2305.15393](https://arxiv.org/abs/2305.15393)
>
> **翻译** [2305.15393](https://hjfy.top/arxiv/2305.15393)
>
> **代码** [Github](https://github.com/weixi-feng/LayoutGPT)

## IdealGPT

> [!NOTE] IdealGPT
> **Arxiv** [2305.14985](https://arxiv.org/abs/2305.14985)
>
> **翻译** [2305.14985](https://hjfy.top/arxiv/2305.14985)
>
> **代码** [Github](https://github.com/Hxyou/IdealGPT)

## Accountable Textual-Visual Chat

> [!NOTE] ATVC
> **Arxiv** [2303.05983](https://arxiv.org/abs/2303.05983)
>
> **翻译** [2303.05983](https://hjfy.top/arxiv/2303.05983)
>
> **代码** [Github](https://github.com/matrix-alpha/Accountable-Textual-Visual-Chat)

## ViperGPT

> [!IMPORTANT] ViperGPT
> **Arxiv** [2303.08128](https://arxiv.org/abs/2303.08128)
>
> **翻译** [2303.08128](https://hjfy.top/arxiv/2303.08128)
>
> **代码** [Github](https://github.com/cvlab-columbia/viper)

## ChatCaptioner

> [!NOTE] ChatCaptioner
> **Arxiv** [2303.06594](https://arxiv.org/abs/2303.06594)
>
> **翻译** [2303.06594](https://hjfy.top/arxiv/2303.06594)
>
> **代码** [Github](https://github.com/Vision-CAIR/ChatCaptioner)

## CaFo

> [!NOTE] CaFo
> **Arxiv** [2303.02151](https://arxiv.org/abs/2303.02151)
>
> **翻译** [2303.02151](https://hjfy.top/arxiv/2303.02151)
>
> **代码** [Github](https://github.com/ZrrSkywalker/CaFo)

## Img2LLM-VQA

> [!NOTE] Img2LLM
> **Arxiv** [2212.10846](https://arxiv.org/abs/2212.10846)
>
> **翻译** [2212.10846](https://hjfy.top/arxiv/2212.10846)
>
> **代码** [Github](https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa)
>
> **Demo** [Demo](https://colab.research.google.com/github/salesforce/LAVIS/blob/main/projects/img2llm-vqa/img2llm_vqa.ipynb)

## SuS-X

> [!NOTE] SuS-X
> **Arxiv** [2211.16198](https://arxiv.org/abs/2211.16198)
>
> **翻译** [2211.16198](https://hjfy.top/arxiv/2211.16198)
>
> **代码** [Github](https://github.com/vishaal27/SuS-X)

## PointCLIP V2

> [!NOTE] PointCLIP V2
> **Arxiv** [2211.11682](https://arxiv.org/abs/2211.11682)
>
> **翻译** [2211.11682](https://hjfy.top/arxiv/2211.11682)
>
> **代码** [Github](https://github.com/yangyangyang127/PointCLIP_V2)

## Socratic Models

> [!IMPORTANT] Socratic Models
> **Arxiv** [2204.00598](https://arxiv.org/abs/2204.00598)
>
> **翻译** [2204.00598](https://hjfy.top/arxiv/2204.00598)
>
> **代码** [Github](https://github.com/google-research/google-research/tree/master/socraticmodels)

> [!IMPORTANT] 待读论文
> 本文档包含LLM辅助视觉推理相关的待读论文列表,持续更新中...
