---
title: "PaperReading: MLLM_2 - Multimodal Instruction Tuning"
timestamp: 2026-01-31 00:00:00+08:00
series: PaperReading
tags: [MLLM, Paper, Instruction Tuning]
description: 多模态指令微调相关论文阅读。
---

### Description

> [!NOTE] 普通论文
> 标准论文记录,蓝色

> [!TIP] 推荐阅读
> 特别推荐的经典论文,绿色

> [!IMPORTANT] 重要突破
> 里程碑式的重要论文,紫色

> [!WARNING] 需注意
> 有争议或需要特别注意的论文,橙色

<br>

## DeepSeek-OCR 2

> [!IMPORTANT] DeepSeek-OCR 2
> **Arxiv** [2026-01-27](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf)
>
> **翻译** [Visual Causal Flow](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf)
>
> **代码** [Github](https://github.com/deepseek-ai/DeepSeek-OCR-2)

## Seed1.8

> [!NOTE] Seed1.8
> **Tech Report** [2025-12-18](https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/research/Seed-1.8-Modelcard.pdf)
>
> **翻译** [Towards Generalized Real-World Agency](https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/research/Seed-1.8-Modelcard.pdf)

## GPT-5.2

> [!IMPORTANT] GPT-5.2
> **OpenAI** [2025-12-11](https://openai.com/index/introducing-gpt-5-2/)
>
> **翻译** [Introducing GPT-5.2](https://openai.com/index/introducing-gpt-5-2/)

## Mistral 3

> [!NOTE] Mistral 3
> **Blog** [2025-12-02](https://mistral.ai/news/mistral-3)
>
> **翻译** [Introducing Mistral 3](https://mistral.ai/news/mistral-3)
>
> **代码** [Huggingface](https://huggingface.co/collections/mistralai/mistral-large-3)

## Qwen3-VL

> [!IMPORTANT] Qwen3-VL
> **Arxiv** [2511.21631](https://arxiv.org/pdf/2511.21631)
>
> **翻译** [2511.21631](https://hjfy.top/arxiv/2511.21631)
>
> **代码** [Github](https://github.com/QwenLM/Qwen3-VL)
>
> **Demo** [Demo](https://huggingface.co/spaces/Qwen/Qwen3-VL-Demo)

## Emu3.5

> [!NOTE] Emu3.5
> **Arxiv** [2510.26583](https://arxiv.org/pdf/2510.26583)
>
> **翻译** [2510.26583](https://hjfy.top/arxiv/2510.26583)
>
> **代码** [Github](https://github.com/baaivision/Emu3.5)

## VITA-E

> [!NOTE] VITA-E
> **Arxiv** [2510.21817](https://arxiv.org/pdf/2510.21817.pdf)
>
> **翻译** [2510.21817](https://hjfy.top/arxiv/2510.21817)
>
> **代码** [Github](https://github.com/Tencent/VITA/tree/VITA-E)
>
> **Demo** Local Demo

## DeepSeek-OCR

> [!NOTE] DeepSeek-OCR
> **Arxiv** [2510.18234](https://arxiv.org/pdf/2510.18234)
>
> **翻译** [2510.18234](https://hjfy.top/arxiv/2510.18234)
>
> **代码** [Github](https://github.com/deepseek-ai/DeepSeek-OCR)

## OmniVinci

> [!NOTE] OmniVinci
> **Arxiv** [2510.15870](https://arxiv.org/pdf/2510.15870)
>
> **翻译** [2510.15870](https://hjfy.top/arxiv/2510.15870)
>
> **代码** [Github](https://github.com/NVlabs/OmniVinci)

## NExT-OMNI

> [!NOTE] NExT-OMNI
> **Arxiv** [2510.13721](https://arxiv.org/pdf/2510.13721)
>
> **翻译** [2510.13721](https://hjfy.top/arxiv/2510.13721)

## InteractiveOmni

> [!NOTE] InteractiveOmni
> **Arxiv** [2510.13747](https://arxiv.org/pdf/2510.13747)
>
> **翻译** [2510.13747](https://hjfy.top/arxiv/2510.13747)
>
> **代码** [Github](https://github.com/SenseTime-FVG/InteractiveOmni)

## VITA-VLA

> [!NOTE] VITA-VLA
> **Arxiv** [2510.09607](https://arxiv.org/pdf/2510.09607)
>
> **翻译** [2510.09607](https://hjfy.top/arxiv/2510.09607)
>
> **代码** [Github](https://github.com/Tencent/VITA/tree/VITA-VLA)

## LLaVA-OneVision-1.5

> [!NOTE] LLaVA-OneVision-1.5
> **Arxiv** [2509.23661](https://arxiv.org/pdf/2509.23661)
>
> **翻译** [2509.23661](https://hjfy.top/arxiv/2509.23661)
>
> **代码** [Github](https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5)
>
> **Demo** [Demo](https://huggingface.co/spaces/lmms-lab/LLaVA-OneVision-1.5)

## Qwen3-Omni

> [!IMPORTANT] Qwen3-Omni
> **Arxiv** [2509.17765](https://arxiv.org/pdf/2509.17765)
>
> **翻译** [2509.17765](https://hjfy.top/arxiv/2509.17765)
>
> **代码** [Github](https://github.com/QwenLM/Qwen3-Omni)
>
> **Demo** [Demo](https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo)

## InternVL3.5

> [!IMPORTANT] InternVL3.5
> **Arxiv** [2508.18265](https://arxiv.org/pdf/2508.18265)
>
> **翻译** [2508.18265](https://hjfy.top/arxiv/2508.18265)
>
> **代码** [Github](https://github.com/OpenGVLab/InternVL)
>
> **Demo** [Demo](https://chat.intern-ai.org.cn/)

## MiniCPM-V 4.5

> [!NOTE] MiniCPM-V 4.5
> **Tech Report** [2025-08-26](https://github.com/OpenBMB/MiniCPM-o)
>
> **代码** [Github](https://github.com/OpenBMB/MiniCPM-o)
>
> **Demo** [Demo](http://101.126.42.235:30910/)

## Thyme

> [!NOTE] Thyme
> **Arxiv** [2508.11630](https://arxiv.org/pdf/2508.11630)
>
> **翻译** [2508.11630](https://hjfy.top/arxiv/2508.11630)
>
> **代码** [Github](https://github.com/yfzhang114/Thyme)
>
> **Demo** [Demo](https://thyme-vl.github.io/)

## GPT-5

> [!IMPORTANT] GPT-5
> **OpenAI** [2025-08-07](https://openai.com/index/introducing-gpt-5/)
>
> **翻译** [Introducing GPT-5](https://openai.com/index/introducing-gpt-5/)

## dots.vlm1

> [!NOTE] dots.vlm1
> **Github** [2025-08-06](https://github.com/rednote-hilab/dots.vlm1)
>
> **代码** [Github](https://github.com/rednote-hilab/dots.vlm1)
>
> **Demo** [Demo](https://huggingface.co/spaces/rednote-hilab/dots-vlm1-demo)

## Step3

> [!NOTE] Step3
> **StepFun** [2025-07-31](https://stepfun.ai/research/step3)
>
> **翻译** [Cost-Effective Multimodal Intelligence](https://stepfun.ai/research/step3)
>
> **代码** [Github](https://github.com/stepfun-ai/Step3)
>
> **Demo** [Demo](https://stepfun.com/)

## GLM-4.1V-Thinking

> [!NOTE] GLM-4.1V-Thinking
> **Arxiv** [2507.01006](https://arxiv.org/pdf/2507.01006)
>
> **翻译** [2507.01006](https://hjfy.top/arxiv/2507.01006)
>
> **代码** [Github](https://github.com/THUDM/GLM-4.1V-Thinking)
>
> **Demo** [Demo](https://huggingface.co/spaces/THUDM/GLM-4.1V-9B-Thinking-API-Demo)

## DenseWorld-1M

> [!NOTE] DenseWorld-1M
> **Arxiv** [2506.24102](https://arxiv.org/pdf/2506.24102)
>
> **翻译** [2506.24102](https://hjfy.top/arxiv/2506.24102)
>
> **代码** [Github](https://github.com/lxtGH/DenseWorld-1M)

## Qwen VLo

> [!NOTE] Qwen VLo
> **Qwen** [2025-06-26](https://qwenlm.github.io/blog/qwen-vlo/)
>
> **翻译** [From Understanding the World to Depicting It](https://qwenlm.github.io/blog/qwen-vlo/)
>
> **Demo** [Demo](https://chat.qwen.ai/)

## MMSearch-R1

> [!NOTE] MMSearch-R1
> **Arxiv** [2506.20670](https://arxiv.org/pdf/2506.20670)
>
> **翻译** [2506.20670](https://hjfy.top/arxiv/2506.20670)
>
> **代码** [Github](https://github.com/EvolvingLMMs-Lab/multimodal-search-r1)

## Show-o2

> [!NOTE] Show-o2
> **Arxiv** [2506.15564](https://arxiv.org/pdf/2506.15564)
>
> **翻译** [2506.15564](https://hjfy.top/arxiv/2506.15564)
>
> **代码** [Github](https://github.com/showlab/Show-o)

## Gemini 2.5

> [!IMPORTANT] Gemini 2.5
> **Google** [2025-06-17](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf)
>
> **翻译** [2025-06-17](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf)

## Ego-R1

> [!NOTE] Ego-R1
> **Arxiv** [2506.13654](https://arxiv.org/pdf/2506.13654)
>
> **翻译** [2506.13654](https://hjfy.top/arxiv/2506.13654)
>
> **代码** [Github](https://github.com/egolife-ai/Ego-R1)

## MiMo-VL

> [!NOTE] MiMo-VL
> **Arxiv** [2506.03569](https://arxiv.org/pdf/2506.03569)
>
> **翻译** [2506.03569](https://hjfy.top/arxiv/2506.03569)
>
> **代码** [Github](https://github.com/XiaomiMiMo/MiMo-VL)

## OpenUni

> [!NOTE] OpenUni
> **Arxiv** [2505.23661](https://arxiv.org/pdf/2505.23661)
>
> **翻译** [2505.23661](https://hjfy.top/arxiv/2505.23661)
>
> **代码** [Github](https://github.com/wusize/OpenUni)

## BAGEL

> [!NOTE] BAGEL
> **Arxiv** [2505.14683](https://arxiv.org/pdf/2505.14683)
>
> **翻译** [2505.14683](https://hjfy.top/arxiv/2505.14683)
>
> **代码** [Github](https://github.com/bytedance-seed/BAGEL)
>
> **Demo** [Demo](https://demo.bagel-ai.org/)

## MMaDA

> [!NOTE] MMaDA
> **Arxiv** [2505.15809](https://arxiv.org/pdf/2505.15809)
>
> **翻译** [2505.15809](https://hjfy.top/arxiv/2505.15809)
>
> **代码** [Github](https://github.com/Gen-Verse/MMaDA)
>
> **Demo** [Demo](https://huggingface.co/spaces/Gen-Verse/MMaDA)

## UniGen

> [!NOTE] UniGen
> **Arxiv** [2505.14682](https://arxiv.org/pdf/2505.14682)
>
> **翻译** [2505.14682](https://hjfy.top/arxiv/2505.14682)

## BLIP3-o

> [!NOTE] BLIP3-o
> **Arxiv** [2505.09568](https://arxiv.org/pdf/2505.09568)
>
> **翻译** [2505.09568](https://hjfy.top/arxiv/2505.09568)
>
> **代码** [Github](https://github.com/JiuhaiChen/BLIP3o)
>
> **Demo** Local Demo

## Seed1.5-VL

> [!NOTE] Seed1.5-VL
> **Arxiv** [2505.07062](https://arxiv.org/pdf/2505.07062)
>
> **翻译** [2505.07062](https://hjfy.top/arxiv/2505.07062)

## Large Multimodal Reasoning Models Survey

> [!NOTE] LMR Survey
> **Arxiv** [2505.04921](https://arxiv.org/pdf/2505.04921)
>
> **翻译** [2505.04921](https://hjfy.top/arxiv/2505.04921)
>
> **代码** [Github](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models)

## VITA-Audio

> [!NOTE] VITA-Audio
> **Arxiv** [2505.03739](https://arxiv.org/pdf/2505.03739)
>
> **翻译** [2505.03739](https://hjfy.top/arxiv/2505.03739)
>
> **代码** [Github](https://github.com/VITA-MLLM/VITA-Audio)
>
> **Demo** Local Demo

## Skywork R1V2

> [!NOTE] Skywork R1V2
> **Arxiv** [2504.16656](https://arxiv.org/pdf/2504.16656)
>
> **翻译** [2504.16656](https://hjfy.top/arxiv/2504.16656)
>
> **代码** [Github](https://github.com/SkyworkAI/Skywork-R1V)

## Eagle 2.5

> [!NOTE] Eagle 2.5
> **Arxiv** [2504.15271](https://arxiv.org/pdf/2504.15271)
>
> **翻译** [2504.15271](https://hjfy.top/arxiv/2504.15271)
>
> **代码** [Github](https://github.com/NVlabs/EAGLE)

## Video Compression LMM

> [!NOTE] Quicksviewer
> **Arxiv** [2504.15270](https://arxiv.org/pdf/2504.15270)
>
> **翻译** [2504.15270](https://hjfy.top/arxiv/2504.15270)
>
> **代码** [Github](https://github.com/quicksviewer/quicksviewer)

## InternVL3

> [!IMPORTANT] InternVL3
> **Arxiv** [2504.10479](https://arxiv.org/abs/2504.10479)
>
> **翻译** [2504.10479](https://hjfy.top/arxiv/2504.10479)
>
> **代码** [Github](https://github.com/OpenGVLab/InternVL)
>
> **Demo** [Demo](https://internvl.opengvlab.com/)

## GPT-4.1

> [!NOTE] GPT-4.1
> **OpenAI** [2025-04-14](https://openai.com/index/gpt-4-1/)
>
> **翻译** [Introducing GPT-4.1 in the API](https://openai.com/index/gpt-4-1/)

## Kimi-VL

> [!NOTE] Kimi-VL
> **Arxiv** [2504.07491](https://arxiv.org/pdf/2504.07491)
>
> **翻译** [2504.07491](https://hjfy.top/arxiv/2504.07491)
>
> **代码** [Github](https://github.com/MoonshotAI/Kimi-VL)
>
> **Demo** [Demo](https://huggingface.co/spaces/moonshotai/Kimi-VL-A3B-Thinking)

## Llama 4

> [!IMPORTANT] Llama 4
> **Meta** [2025-04-05](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)
>
> **翻译** [2025-04-05](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)
>
> **代码** [Hugging Face](https://huggingface.co/collections/meta-llama/llama-4-67f0c30d9fe03840bc9d0164)

## Qwen2.5-Omni

> [!IMPORTANT] Qwen2.5-Omni
> **Qwen** [2025-03-26](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf)
>
> **翻译** [2025-03-26](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf)
>
> **代码** [Github](https://github.com/QwenLM/Qwen2.5-Omni)
>
> **Demo** [Demo](https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo)

## GPT-4o Image Generation

> [!NOTE] GPT-4o Image
> **OpenAI** [2025-03-25](https://cdn.openai.com/11998be9-5319-4302-bfbf-1167e093f1fb/Native_Image_Generation_System_Card.pdf)
>
> **翻译** [Native Image Generation](https://cdn.openai.com/11998be9-5319-4302-bfbf-1167e093f1fb/Native_Image_Generation_System_Card.pdf)

## Sparrow

> [!NOTE] Sparrow
> **Arxiv** [2411.19951](https://arxiv.org/pdf/2411.19951)
>
> **翻译** [2411.19951](https://hjfy.top/arxiv/2411.19951)
>
> **代码** [Github](https://github.com/VITA-MLLM/Sparrow)

## Nexus-O

> [!NOTE] Nexus-O
> **Arxiv** [2503.01879](https://arxiv.org/pdf/2503.01879)
>
> **翻译** [2503.01879](https://hjfy.top/arxiv/2503.01879)

## Phi-4-Mini

> [!NOTE] Phi-4-Mini
> **Arxiv** [2503.01743](https://arxiv.org/pdf/2503.01743)
>
> **翻译** [2503.01743](https://hjfy.top/arxiv/2503.01743)
>
> **代码** [Hugging Face](https://huggingface.co/microsoft/Phi-4-multimodal-instruct)
>
> **Demo** [Demo](https://huggingface.co/spaces/microsoft/phi-4-multimodal)

## Long-VITA

> [!NOTE] Long-VITA
> **Arxiv** [2502.05177](https://arxiv.org/pdf/2502.05177)
>
> **翻译** [2502.05177](https://hjfy.top/arxiv/2502.05177)
>
> **代码** [Github](https://github.com/VITA-MLLM/Long-VITA)

## Qwen2.5-VL

> [!IMPORTANT] Qwen2.5-VL
> **Arxiv** [2502.13923](https://arxiv.org/pdf/2502.13923)
>
> **翻译** [2502.13923](https://hjfy.top/arxiv/2502.13923)
>
> **代码** [Github](https://github.com/QwenLM/Qwen2.5-VL)
>
> **Demo** [Demo](https://huggingface.co/spaces/Qwen/Qwen2.5-VL)

## Baichuan-Omni-1.5

> [!NOTE] Baichuan-Omni-1.5
> **Tech Report** [2025-01-26](https://github.com/baichuan-inc/Baichuan-Omni-1.5/blob/main/baichuan_omni_1_5.pdf)
>
> **翻译** [2025-01-26](https://github.com/baichuan-inc/Baichuan-Omni-1.5/blob/main/baichuan_omni_1_5.pdf)
>
> **代码** [Github](https://github.com/baichuan-inc/Baichuan-Omni-1.5)
>
> **Demo** Local Demo

## LlamaV-o1

> [!NOTE] LlamaV-o1
> **Arxiv** [2501.06186](https://arxiv.org/pdf/2501.06186)
>
> **翻译** [2501.06186](https://hjfy.top/arxiv/2501.06186)
>
> **代码** [Github](https://github.com/mbzuai-oryx/LlamaV-o1)

## VITA-1.5

> [!IMPORTANT] VITA-1.5
> **Arxiv** [2501.01957](https://arxiv.org/pdf/2501.01957)
>
> **翻译** [2501.01957](https://hjfy.top/arxiv/2501.01957)
>
> **代码** [Github](https://github.com/VITA-MLLM/VITA)

## QVQ

> [!NOTE] QVQ
> **Qwen** [2024-12-25](https://qwenlm.github.io/blog/qvq-72b-preview/)
>
> **翻译** [To See the World with Wisdom](https://qwenlm.github.io/blog/qvq-72b-preview/)
>
> **代码** [Github](https://github.com/QwenLM/Qwen2-VL)
>
> **Demo** [Demo](https://qwenlm.github.io/blog/qvq-72b-preview/)

## DeepSeek-VL2

> [!IMPORTANT] DeepSeek-VL2
> **Arxiv** [2412.10302](https://arxiv.org/pdf/2412.10302)
>
> **翻译** [2412.10302](https://hjfy.top/arxiv/2412.10302)
>
> **代码** [Github](https://github.com/deepseek-ai/DeepSeek-VL2)

## Apollo

> [!NOTE] Apollo
> **Arxiv** [2412.10360](https://arxiv.org/pdf/2412.10360)
>
> **翻译** [2412.10360](https://hjfy.top/arxiv/2412.10360)

## InternLM-XComposer2.5-OmniLive

> [!NOTE] XComposer2.5-OmniLive
> **Arxiv** [2412.09596](https://arxiv.org/pdf/2412.09596)
>
> **翻译** [2412.09596](https://hjfy.top/arxiv/2412.09596)
>
> **代码** [Github](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive)
>
> **Demo** Local Demo

## StreamChat

> [!NOTE] StreamChat
> **Arxiv** [2412.08646](https://arxiv.org/pdf/2412.08646)
>
> **翻译** [2412.08646](https://hjfy.top/arxiv/2412.08646)

## CompCap

> [!NOTE] CompCap
> **Arxiv** [2412.05243](https://arxiv.org/pdf/2412.05243)
>
> **翻译** [2412.05243](https://hjfy.top/arxiv/2412.05243)

## LinVT

> [!NOTE] LinVT
> **Arxiv** [2412.05185](https://arxiv.org/pdf/2412.05185)
>
> **翻译** [2412.05185](https://hjfy.top/arxiv/2412.05185)
>
> **代码** [Github](https://github.com/gls0425/LinVT)

## InternVL Performance Scaling

> [!NOTE] InternVL Scaling
> **Arxiv** [2412.05271](https://arxiv.org/pdf/2412.05271)
>
> **翻译** [2412.05271](https://hjfy.top/arxiv/2412.05271)
>
> **代码** [Github](https://github.com/OpenGVLab/InternVL)
>
> **Demo** [Demo](https://internvl.opengvlab.com)

## NVILA

> [!NOTE] NVILA
> **Arxiv** [2412.04468](https://arxiv.org/pdf/2412.04468)
>
> **翻译** [2412.04468](https://hjfy.top/arxiv/2412.04468)
>
> **代码** [Github](https://github.com/NVlabs/VILA)
>
> **Demo** [Demo](https://vila.mit.edu)

## Inst-IT

> [!NOTE] Inst-IT
> **Arxiv** [2412.03565](https://arxiv.org/pdf/2412.03565)
>
> **翻译** [2412.03565](https://hjfy.top/arxiv/2412.03565)
>
> **代码** [Github](https://github.com/inst-it/inst-it)

## TimeMarker

> [!NOTE] TimeMarker
> **Arxiv** [2411.18211](https://arxiv.org/pdf/2411.18211)
>
> **翻译** [2411.18211](https://hjfy.top/arxiv/2411.18211)
>
> **代码** [Github](https://github.com/TimeMarker-LLM/TimeMarker/)

## ChatRex

> [!NOTE] ChatRex
> **Arxiv** [2411.18363](https://arxiv.org/pdf/2411.18363)
>
> **翻译** [2411.18363](https://hjfy.top/arxiv/2411.18363)
>
> **代码** [Github](https://github.com/IDEA-Research/ChatRex)
>
> **Demo** Local Demo

> [!IMPORTANT] 待读论文
> 本文档包含多模态指令微调相关的待读论文列表,持续更新中...
